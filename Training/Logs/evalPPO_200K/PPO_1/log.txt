Logging to Training/Logs/evalPPO_200K/PPO_1
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | -10.1    |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.2     |
|    mean_reward     | -10.1    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | -10.1    |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.4     |
|    mean_reward     | -10.1    |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 85.3       |
|    ep_rew_mean     | -10.942737 |
| time/              |            |
|    fps             | 206        |
|    iterations      | 1          |
|    time_elapsed    | 9          |
|    total_timesteps | 2048       |
-----------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | -19.7      |
| time/                   |            |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.01105582 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -14.2      |
|    explained_variance   | 0.00553    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0149    |
|    std                  | 0.999      |
|    value_loss           | 5.83       |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | -26.2    |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -14.2    |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | -32.1    |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 79         |
|    ep_rew_mean     | -10.710744 |
| time/              |            |
|    fps             | 144        |
|    iterations      | 2          |
|    time_elapsed    | 28         |
|    total_timesteps | 4096       |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | -34.1       |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.008929359 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | -0.0266     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.01        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0103     |
|    std                  | 1           |
|    value_loss           | 4.04        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | -36      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | -20.1    |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -44.9    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 108        |
|    ep_rew_mean     | -12.948359 |
| time/              |            |
|    fps             | 130        |
|    iterations      | 3          |
|    time_elapsed    | 47         |
|    total_timesteps | 6144       |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | -35.8       |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.011506986 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | -0.114      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.35        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00781    |
|    std                  | 1           |
|    value_loss           | 9.41        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -40.1    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | -32.3    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -28.6    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 111        |
|    ep_rew_mean     | -14.992903 |
| time/              |            |
|    fps             | 125        |
|    iterations      | 4          |
|    time_elapsed    | 65         |
|    total_timesteps | 8192       |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | -21.8       |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.009177218 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | -0.0659     |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00939    |
|    std                  | 1           |
|    value_loss           | 32.9        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | -28.1    |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | -35.1    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 130        |
|    ep_rew_mean     | -16.845259 |
| time/              |            |
|    fps             | 126        |
|    iterations      | 5          |
|    time_elapsed    | 80         |
|    total_timesteps | 10240      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | -40.9       |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.015342329 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0113     |
|    std                  | 1           |
|    value_loss           | 12.6        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | -39.4    |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -35.1    |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | -15.4    |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 148        |
|    ep_rew_mean     | -17.173529 |
| time/              |            |
|    fps             | 123        |
|    iterations      | 6          |
|    time_elapsed    | 99         |
|    total_timesteps | 12288      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 148         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.008433276 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.0151      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.19        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00716    |
|    std                  | 1           |
|    value_loss           | 48.7        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | -20.2    |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | -8.58    |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | -19.6    |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 151        |
|    ep_rew_mean     | -12.098553 |
| time/              |            |
|    fps             | 123        |
|    iterations      | 7          |
|    time_elapsed    | 115        |
|    total_timesteps | 14336      |
-----------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 164          |
|    mean_reward          | -8.78        |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0053757895 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | -0.00269     |
|    learning_rate        | 0.0003       |
|    loss                 | 61.7         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00363     |
|    std                  | 1.01         |
|    value_loss           | 153          |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | -27.6    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | -21.2    |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | -22.4    |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 153        |
|    ep_rew_mean     | -11.076711 |
| time/              |            |
|    fps             | 124        |
|    iterations      | 8          |
|    time_elapsed    | 131        |
|    total_timesteps | 16384      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | -16.4       |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.010889229 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00686    |
|    std                  | 0.999       |
|    value_loss           | 31.8        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | -23.5    |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | -17.4    |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | -17.4    |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 179        |
|    ep_rew_mean     | -15.133371 |
| time/              |            |
|    fps             | 127        |
|    iterations      | 9          |
|    time_elapsed    | 144        |
|    total_timesteps | 18432      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | -26.6       |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.012628155 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.76        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0112     |
|    std                  | 1           |
|    value_loss           | 13.6        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | -15.9    |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | -24.4    |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | -14.6    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 196        |
|    ep_rew_mean     | -16.733883 |
| time/              |            |
|    fps             | 131        |
|    iterations      | 10         |
|    time_elapsed    | 156        |
|    total_timesteps | 20480      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 3.72        |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.009789333 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.9        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00889    |
|    std                  | 1           |
|    value_loss           | 17.1        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | -14      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | -17.4    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.2     |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 197        |
|    ep_rew_mean     | -16.381678 |
| time/              |            |
|    fps             | 132        |
|    iterations      | 11         |
|    time_elapsed    | 170        |
|    total_timesteps | 22528      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.2        |
|    mean_reward          | -32.7       |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.009402946 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.0592      |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00673    |
|    std                  | 1           |
|    value_loss           | 53          |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | -34.2    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | -29.1    |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | -25.8    |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 209        |
|    ep_rew_mean     | -18.728695 |
| time/              |            |
|    fps             | 134        |
|    iterations      | 12         |
|    time_elapsed    | 183        |
|    total_timesteps | 24576      |
-----------------------------------
