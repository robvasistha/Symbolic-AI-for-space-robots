Logging to Training/Logs/PPO_200Klegacy/_1
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 75.8       |
|    ep_rew_mean     | -11.521986 |
|    reward          | -311       |
| time/              |            |
|    fps             | 243        |
|    iterations      | 1          |
|    time_elapsed    | 8          |
|    total_timesteps | 2048       |
-----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 87          |
|    ep_rew_mean          | -10.731576  |
|    reward               | -193        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 2           |
|    time_elapsed         | 17          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008779613 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.027       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.98        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0136     |
|    std                  | 1           |
|    value_loss           | 14.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.8        |
|    ep_rew_mean          | -11.707653  |
|    reward               | -327        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 3           |
|    time_elapsed         | 26          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012044472 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.93        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0172     |
|    std                  | 1           |
|    value_loss           | 3.17        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.7        |
|    ep_rew_mean          | -11.499885  |
|    reward               | -112        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 4           |
|    time_elapsed         | 35          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008588528 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.45        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0101     |
|    std                  | 0.999       |
|    value_loss           | 17          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -12.027706  |
|    reward               | -247        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 5           |
|    time_elapsed         | 44          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.012106954 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.996       |
|    value_loss           | 0.682       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | -11.706398  |
|    reward               | -141        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 6           |
|    time_elapsed         | 53          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.007913246 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.1       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.99        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00629    |
|    std                  | 0.995       |
|    value_loss           | 13.5        |
-----------------------------------------
