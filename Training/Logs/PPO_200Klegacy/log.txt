Logging to Training/Logs/PPO_200Klegacy
-----------------------------------
| rollout/           |            |
|    ep_len_mean     | 59.4       |
|    ep_rew_mean     | -11.203337 |
|    reward          | -381       |
| time/              |            |
|    fps             | 246        |
|    iterations      | 1          |
|    time_elapsed    | 8          |
|    total_timesteps | 2048       |
-----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 68          |
|    ep_rew_mean          | -11.917788  |
|    reward               | -334        |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 2           |
|    time_elapsed         | 17          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010878222 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.0021      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.57        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0137     |
|    std                  | 0.999       |
|    value_loss           | 16.1        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 75.8        |
|    ep_rew_mean          | -10.02744   |
|    reward               | -55.6       |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 3           |
|    time_elapsed         | 26          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008108255 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.0874      |
|    learning_rate        | 0.0003      |
|    loss                 | 16.8        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0125     |
|    std                  | 1           |
|    value_loss           | 17.6        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 82.5         |
|    ep_rew_mean          | -9.977701    |
|    reward               | -207         |
| time/                   |              |
|    fps                  | 233          |
|    iterations           | 4            |
|    time_elapsed         | 35           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0066804695 |
|    clip_fraction        | 0.0572       |
|    clip_range           | 0.2          |
|    entropy_loss         | -14.2        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.913        |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00567     |
|    std                  | 1            |
|    value_loss           | 45.2         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.3        |
|    ep_rew_mean          | -9.510632   |
|    reward               | -154        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 5           |
|    time_elapsed         | 44          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011084466 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.849       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0127     |
|    std                  | 1.01        |
|    value_loss           | 1.8         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | -8.269672   |
|    reward               | -66.9       |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 6           |
|    time_elapsed         | 53          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012884399 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0186     |
|    std                  | 1.01        |
|    value_loss           | 0.985       |
-----------------------------------------
